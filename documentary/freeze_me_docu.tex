\documentclass[12pt]{scrartcl}
\usepackage[german]{babel}
\usepackage{graphicx}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{color}
\usepackage[linkcolor=black, urlcolor=black, citecolor=dblue,
breaklinks, bookmarks, colorlinks]{hyperref}

\begin {document}

\hspace{-0.4cm}\huge \textbf{Freeze Me! - Dokumentation}\vspace{3pt}\\\Large A Media Processing Project \vspace{14pt}\large 

\noindent
\textbf{Stand:} 28.02.2023 \\

\vspace{3pt} \normalsize Gruppe: 
Jan Rekemeyer, Iskander Yusupov und Hendrik Finke

\section{Motivation}
Da das Ziel des Projekts darin bestand, ein Softwareprodukt zu erstellen, entschied sich die Gruppe, eine Motivation im Marketingformat zu erstellen.\\Gl\"uckliche Momente lassen sich in einem Foto festhalten, das dann verschenkt werden kann. Aber was tun mit den gl\"ucklichen Momenten, die auf Video festgehalten wurden? Mit diesem Ansatz suchte die Gruppe nach M\"oglichkeiten, ein Video so in Form eines Bildes festzuhalten, dass dieses sp\"ater beispielsweise als gerahmtes Geschenk pr\"asentiert werden k\"onnte.
Eine weitere Inspiration f\"ur unsere Gruppe waren Videos mit viel Dynamik. Sport, Tanz, aber auch Tiktok-Challenges enthalten viel Bewegung. Beim Fotografieren von sich bewegenden Objekten erzeugt die Einstellung einer Langzeitbeleuchtung den Effekt einer \glqq eingefrorenen\grqq  Bewegung. Diese Fotos haben ihre eigene \"Asthetik und unsere Gruppe wollte versuchen, diesen Effekt mit den im Modul erlernten F\"ahigkeiten zu reproduzieren.

\section{Zielsetzung}Unser Projekt haben wir der folgende Zielsetzung gewidmet: \\
\textit{Wir entwickeln ein Programm, welches es erm\"oglicht aus einem Video ein Bild zu erzeugen, welches die Bewegungsdynamik des Videos \"asthetisch einfasst und dabei den Eindruck fortw\"ahrender Bewegung erh\"alt.}

\section{Langzeitbelichtung - Inspirationsquelle} %Erstmal Hendriks Sektion
\subsection{Motivation}Das in den Er\"offnungsfolien gezeigte Bildmaterial erinnerte uns in Teilen doch recht stark an eine klassische
Langzeitbelichtung, wie
man sie aus der Analogfotografie oder der Fotographie mit DSLRs oder DSLMs kennt. Daher stellten wir uns die Frage, wie man dieses Konzept digital nachtr\"aglich unter Zuhilfenahme eines Videos anstelle eines Fotos reproduzieren kann. \\
\subsection{Funktionsweise}Der Ansatz, den wir hierbei Verfolgt haben ist relativ trivial. \"Ahnlich wie klassiche Filter den
Inhaltswert eines Pixels anhand einer ggf. faktorisierten Durchschnittsberechnung mit den umliegenden Pixeln berechnet tun wir ebendieses - jedoch mit den Pixeln gleicher Koordinaten auf anderen Videoframes. Zu diesem Zwecke werden Videos Frame f\"ur Frame betrachtet und dann Pixel f\"ur Pixel in ihre jeweiligen 3 Farbkomponenten zerlegt. Im folgenden wird der Durchschnittswert der Farbwerte errechnet und somit eine Art Langzeitpixel erzeugt, deren Summe das k\"unstlich Langzeitbelichtete Bild ergibt.
 \subsection{Resultate}Die Resultate waren abh\"angig vom eingegebenen Video durchaus vielversprechend. Ein von uns anf\"anglich eigens erstelltes Video von der Kreuzung der Ammerl\"ander Heerstra{\ss}e mit dem Uhlhornsweg resultierte in einem mehr oder minder matschigen Bild. Das Ergebnis des Contemporary Dancers hingegen war schon erstaunlich dicht an den Hintergrunderscheinungen der Fotos in den Veranstaltungsfolien. Auch das TikTok Video mit dem Sprung resultierte in einem relativ ansehnlichen Ergebnis. Da dieser Ansatz somit der erste nennenswerte Schritt in die Richtung des Gesamtzieles war wurde er beibehalten und die Langzeitbelichtung
wurde somit Grundkomponente unseres Projektes.
% Anforderungsanalyse

% Langzeitbelichtung
% -> Motivation, Theorie, Codebeschreibung, Resultate
% Grabcut mit Thresholding
% -> Motivation, Theorie, Codebeschreibung, Resultate
% Canny-Edge Detection

\section{Edge Detection}
\subsection{Motivation}
Nachdem es der Gruppe gelungen war, Bilder mit einer k\"unstlichen Langzeitblichtung aus dem Video zu erstellen, fiel ein Merkmal auf. Wurde das Video ohne Stativ oder jegliche Art von Unterst\"utzung gedreht, dann f\"uhrte das leichte Wackeln der Kamera irgendwann dazu, dass sich auch stabile Objekte im Ausgabebild als unscharf herausstellten. Um dieses Merkmal oder Problem zu l\"osen, wurde entschieden, Canny Edge Detection zu verwenden.
Die Idee, der Algorithmus zu verwenden, wurde experimentell geboren. Wir haben versucht, Canny Edge Detection auf ein bereits bearbeitetes Bild anzuwenden. Die Methode erkannte nur Objekte als Kanten, die auf dem Video stabil waren. Dann entschied die Gruppe, dass es m\"oglich ist, Sch\"arfungsverfahren (z. B. einen Laplace-Filter) auf die Pixelkoordinaten mit stabilen Objekten anzuwenden, die mithilfe des Canny Edge Detection auf dem Ausgabebild erhalten werden.
\subsection{Theorie}
Der Canny Edge Detection ist ein mehrstufiger Algorithmus zum Erkennen von Kanten in einem Bild. Der Algorithmus besteht aus vier Stufen. In der ersten Stufe ist es notwendig, Rauschen aus dem Originalbild zu entfernen, f\"ur diese Aufgabe wird der Gau{\ss} Filter angewendet. Der zweite Stufe besteht darin, den Gr\"o{\ss}e entlang der x- und y-Dimension zu erhalten. Dazu wird die Ableitung des Gau{\ss}-Filters berechnet, um dann das Gradient der Bildpixel zu berechnen.
Der Algorithmus geht alle Punkte der Gradientenintensit\"atsmatrix durch und findet die Pixel mit dem maximalen Wert in den Kantenrichtungen.
Das Betrachten der Gruppe von Nachbarn f\"ur jede Kurve in einer Richtung senkrecht zu der gegebenen Kante unterdr\"uckt nicht maximalen Kantenbeitragspixelpunkte. Schlie{\ss}lich wird das Hysterese-Schwellenwertverfahren verwendet, um die Pixel zu bewahren, die h\"oher als die Gradientengr\"o{\ss}e sind, und diejenigen zu vernachl\"assigen, die niedriger als der niedrige Schwellenwert sind.
\subsection{Codebeschreibung}
Zur Optimierung der Ergebnisse wird ein Verfahren zur Bestimmung der Schwellwerte (minVal und maxVal) eingesetzt. Die Schwellwerte werden dann in der OpenCV Methode cv.Canny \"ubergegeben.\\
\subsection{Ergebnis}
Nach dem Ausf\"uhren des Codes ist das erwartete Ergebnis ein Bild mit identifizierten Kanten. Nach Auswertung der Ergebnisse entschied die Gruppe, dass der resultierende Bildsch\"arfungseffekt nicht effektiv genug war, und entschied sich daher, Canny Edge Detection aufzugeben.\\
\section{Background Subtraction}
\subsection{Motivation}
Die Gruppe stand vor dem Problem, dass manchmal ein sich bewegendes Objekt im Video nach Anwendung der k\"unstlichen Langzeitblichtung weniger unterscheidbar wurde. Um Kontrast und Transparenz hinzuzuf\"ugen, wurde entschieden, dem endg\"ultigen Bild ein sich bewegende Objekt hinzuzuf\"ugen, das aus einem der Videoframes geschnitten wurde.
Zu diesem Zweck entschied sich die Gruppe, zwei Background- und Foregroundsubtraction Methoden von OpenCV anzuwenden:K-Nearest Neighbors (KNN) und Gaussian Mixture-based (MOG2).\\
\subsection{Theorie KNN}
Der erste Schritt besteht darin, die Anzahl K der Nachbarn zu w\"ahlen. Dann ist es notwendig, den euklidischen Abstand der Anzahl von K Nachbarn zu berechnen. Basierend auf der berechneten euklidischen euklidischen Distanz werden K n\"achste Nachbarn ausgew\"ahlt. Unter diesen k Nachbarn wird die Anzahl der Datenpunkte in jeder Kategorie gez\"ahlt.
Am Ende werden neue Datenpunkte der Kategorie zugeordnet, f\"ur die die Anzahl der Nachbarn maximal ist.
\subsection{Theorie MOG2}
Die Mix of Gaussian (MOG)-Hintergrundsubtraktion ist abh\"angig von einer Kombination von Frames anstelle von nur einem Frame. Bei diesem Verfahren wird f\"ur jedes Hintergrundpixel eine Mischung aus einer Gau{\ss}schen Verteilung k und einem Gewichtungsparameter verwendet, um die Lebensdauer von Pixeln in der Szene zu speichern, wobei k im Bereich von 3 bis 5 variieren kann. Verbleibende Pixel mit einer Zeit gr\"o{\ss}er als a Schwellen in der Szene haben eine h\"ohere Wahrscheinlichkeit, zur Hintergrundszene zu geh\"oren. Wenn das Pixel f\"ur einen bestimmten Zeitraum unver\"andert bleibt, wird es als dominantes Hintergrundpixel betrachtet. Wenn die Differenz von Pixeln in einem Frame gr\"o{\ss}er als ein vordefinierter Schwellenwert ist, werden sie als bewegliche Teile klassifiziert. Diese Methode reagiert sehr empfindlich auf Ver\"anderungen in der Umgebung.
In unserem Projekt wird MOG2 verwendet. Der Unterschied besteht darin, dass MOG2 die geeignete Anzahl von Gau{\ss}schen Verteilungen f\"ur jedes Pixel ausw\"ahlt, wobei MOG eine K-Gau{\ss}sche Verteilung f\"ur die Modellierung verwendet. Aus diesem Grund bietet MOG2 eine bessere Anpassungsf\"ahigkeit an wechselnde Szenen aufgrund von Beleuchtungs\"anderungen.
\subsection{Codebeschreibung}
Der Code der Funktion “get_foreground_mask” \"uberpr\"uft das Argument "foregroundMaskOption", um zu bestimmen, ob die Methode "cv.createBackgroundSubtractorMOG2()" oder "cv.createBackgroundSubtractorKNN()" f\"ur die Hintergrundsubtraktion verwendet werden soll. Der n\"achste Schritt besteht darin, eine URL zum Herunterladen des angegebenen Videos mit videoId und cldId als Parameter zu erstellen. Danach l\"adt es das Video mit urllib.request.urlretrieve() von der URL auf einen lokalen Pfad herunter. Dann wird das VideoCapture-Objekt aus dem heruntergeladenen Video erstellt. F\"ur jeden Frame des Videos wird eine Hintergrundsubtraktion angewendet. Die resultierende Vordergrundmaske wird in einer Datei gespeichert.
Der Code der Funktion “get_background_image” ist \"ahnlich geschrieben, speichert aber auch Hintergrundbild.\\
\subsection{Ergebnis}
Das Ergebnis beider Methoden war f\"ur die Aufgabenstellung nicht geeignet. Die unter Verwendung dieser beiden Algorithmen erhaltenen Masken waren nicht monolithisch, sondern hatten L\"ucken darin. Eine Maske mit L\"ucken war nicht tauglich.
Daher entschied sich die Gruppe f\"ur eine andere Methode f\"ur Hintergrund- und Vordergrundsubtraktion.\\
% MOG2 und KNN
% -> Motivation, Theorie, Codebeschreibung, Resultate

% Frontend: Primer; Paar Buttons eingebaut; Backend mit Funktionen ausgestattet

% Probleme: Coco Dataset,  HED, Django

% Fazit: Limitierungen
% Ausblick: Verbesserungsvorschl\"age



\section{Fazit}
Das letztendliche Ergebnis unserer Arbeit entspricht unser Meinung nach dem urspr\"unglichen Ziel - aufgrund der Subjektivit\"at der
\"Asthetik aber l\"asst sich dieses Urteil wohl kaum objektiv f\"allen. Mit der Anwendung der Langzeitbelichtung fand sich f\"ur uns relativ
fr\"uh eine M\"oglichkeit, einen Hintergrund mit den verwaschenen bzw. verschwommenen Hintergrundartefakten zu erzeugen.
\subsection{Limitierungen}
Die Erkennung des Vordergrundes beschr\"ankt die Funktionsweise der Software in Teilen. Von den anf\"anglich 3 angenommenen Testvideos
$ ( $T\"anzer, Kreuzung, TikTok Sprungvideo$ ) $ funktioniert lediglich der T\"anzer wirklich gut. Das Ergebnis des TikTok Sprungvideos ist
ebenfalls durchaus annehmbar, wobei hier das Hinzuf\"ugen eines maskierten Standbildes weitestgehend wertlos ist. Zu diesem Zwecke m\"usste ein Framepicker in der Software implementiert werden. Gleicherma{\ss}en ist der Kontrast der Hintergrundeffekte relativ stark beschr\"ankt, was ein Betrachten der Resultate in schlechten Lichtverh\"altnissen oder gar auf Beamern stark einschr\"ankt. Um diesen Effekt zu verst\"arken h\"atte eine helligkeitsbedingte Faktorisierung genutzt werden k\"onnen, die wir anf\"anglich mal betrachtet haben, deren Notwendigkeit uns im Laufe des Projektes nicht mehr sinnvoll erschien. Retrospektiv w\"are diese Funktion ein sinniges Opt-In gewesen.
\subsection{Ausbaum\"oglichkeiten}
F\"ur eine k\"unftige Ausgestaltung bieten sich die im letzten Block genannten Optionen an: Eine Optimierung des Interfaces um ein Frame zu selektieren, die Option im entsprechenden Frame ein Rechteck zu w\"ahlen, das dann an GrabCut \"ubergeben wird um den Vordergrund zu bestimmen. Dazu w\"are es eine sicherlich gute Option gewesen die Lichtst\"arke in die Langzeitbelichtung einflie{\ss}en zu lassen, um den Kontrast der Schlieren im Hintergrund zu steigern.

\end{document}
